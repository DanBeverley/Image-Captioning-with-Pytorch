{"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1111676,"sourceType":"datasetVersion","datasetId":623289}],"dockerImageVersionId":30674,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true},"colab":{"provenance":[{"file_id":"https://storage.googleapis.com/kaggle-colab-exported-notebooks/image-captioning-9a5caccb-523e-43e4-94de-9f5098b339fa.ipynb?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com/20240408/auto/storage/goog4_request&X-Goog-Date=20240408T015501Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=13fd02685342bb62a637c92937d5ff47e5a64ca16f034e42a1e42c1c67189fbbeb4c80edd716bfb8ae4a19b75e87b50803055ddf08a524e4e3b85a34af4cefe0f450515dc06dd2bb957a54295d8e3fac815874866105b69ed95a8121fcde8c7201fc1c6116a377fa1db75f51c30a64b4bb26ba02a6272c177944bf98e1eb7170636289d042c7302ceb20cce25e8c9102b201c86149d2b67093a2e8ac09bd9010ac9c659421d7d4419579b02c2383660c16a3720413ac00f28e14cacbf3be82b79be3193edf9f2ef194c46a8c35f8cb838e16f6bd85b2c5294762034848c3ecc0c5a7a5acb2244d7923ea72a837d8ac475845e2daf98e3d96dbd59e3a23f74772","timestamp":1712558493720}],"gpuType":"T4"},"accelerator":"GPU"},"nbformat_minor":0,"nbformat":4,"cells":[{"source":["\n","# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n","# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n","# THEN FEEL FREE TO DELETE THIS CELL.\n","# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n","# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n","# NOTEBOOK.\n","\n","import os\n","import sys\n","from tempfile import NamedTemporaryFile\n","from urllib.request import urlopen\n","from urllib.parse import unquote, urlparse\n","from urllib.error import HTTPError\n","from zipfile import ZipFile\n","import tarfile\n","import shutil\n","\n","CHUNK_SIZE = 40960\n","DATA_SOURCE_MAPPING = 'flickr8k:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F623289%2F1111676%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240408%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240408T015501Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Dd7e0830dc0dddea0f7f6a886d269b5aaf59d20dd507250b4b7bc1de498f04532d64419286b0c62d9a2dd3cf97d852544f10014f04f180f3a4a920e53d0de082ffc1913c55b827c7b7abaeeedfeb1b725e215d94abe1e8e407afd4a239a437943ecc89a8308827992dbd606264d2472f3b8864727589584b905ce902937449c2bd437c1f5445e01e51460b056c6141a8bc23922409430bedd181d3d159ce070bb310dac5d6419c97f52168bf4cb52e709bb3d3104b997e9d27f84421512a686f8eee9fa08ae073b4189d4f83523098da116e37fe707d4db20ae7cf95a30464d3f0ef276685517de84979eaaaf290dd2a20042a764c4f2d4f3ef2a5607022953a2'\n","\n","KAGGLE_INPUT_PATH='/kaggle/input'\n","KAGGLE_WORKING_PATH='/kaggle/working'\n","KAGGLE_SYMLINK='kaggle'\n","\n","!umount /kaggle/input/ 2> /dev/null\n","shutil.rmtree('/kaggle/input', ignore_errors=True)\n","os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n","os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n","\n","try:\n","  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n","except FileExistsError:\n","  pass\n","try:\n","  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n","except FileExistsError:\n","  pass\n","\n","for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n","    directory, download_url_encoded = data_source_mapping.split(':')\n","    download_url = unquote(download_url_encoded)\n","    filename = urlparse(download_url).path\n","    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n","    try:\n","        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n","            total_length = fileres.headers['content-length']\n","            print(f'Downloading {directory}, {total_length} bytes compressed')\n","            dl = 0\n","            data = fileres.read(CHUNK_SIZE)\n","            while len(data) > 0:\n","                dl += len(data)\n","                tfile.write(data)\n","                done = int(50 * dl / int(total_length))\n","                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n","                sys.stdout.flush()\n","                data = fileres.read(CHUNK_SIZE)\n","            if filename.endswith('.zip'):\n","              with ZipFile(tfile) as zfile:\n","                zfile.extractall(destination_path)\n","            else:\n","              with tarfile.open(tfile.name) as tarfile:\n","                tarfile.extractall(destination_path)\n","            print(f'\\nDownloaded and uncompressed: {directory}')\n","    except HTTPError as e:\n","        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n","        continue\n","    except OSError as e:\n","        print(f'Failed to load {download_url} to path {destination_path}')\n","        continue\n","\n","print('Data source import complete.')\n"],"metadata":{"id":"52ZVcjbTaomL","executionInfo":{"status":"ok","timestamp":1712553184410,"user_tz":-420,"elapsed":82224,"user":{"displayName":"","userId":""}},"outputId":"96308d1a-8b47-4733-b61a-6c6646e51779","colab":{"base_uri":"https://localhost:8080/"}},"cell_type":"code","outputs":[{"output_type":"stream","name":"stdout","text":["Downloading flickr8k, 1112971163 bytes compressed\n","[==================================================] 1112971163 bytes downloaded\n","Downloaded and uncompressed: flickr8k\n","Data source import complete.\n"]}],"execution_count":null},{"cell_type":"code","source":["#Loading in Flickr8k Dataset\n","import torch\n","import pandas as pd\n","from torch import nn\n","import torchvision\n","from PIL import Image\n","from torchvision import transforms , datasets\n","from torch.utils.data import DataLoader , Dataset\n","import matplotlib.pyplot as plt\n","from torchvision.models import resnet50, ResNet50_Weights\n","import torch.nn.functional as F\n","from torchvision.transforms import v2\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","#transform = transforms.Compose([transforms.RandomResizedCrop(224 , scale = (0.9 , 1.0) , ratio = (0.9 , 1.1)),\n","#                                transforms.CenterCrop(224),\n","#                                transforms.RandomHorizontalFlip(),\n","#                                transforms.ToTensor(),\n","#                                transforms.Normalize(mean = [0.486 , 0.456 , 0.406] , std = [0.229 , 0.224 , 0.225])])\n","transform = v2.Compose([\n","    v2.Resize((224, 224)),\n","    v2.PILToTensor(),\n","    v2.ToDtype(torch.float32, scale=True)\n","])\n","print(device)"],"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-08T01:50:46.473016Z","iopub.execute_input":"2024-04-08T01:50:46.473903Z","iopub.status.idle":"2024-04-08T01:50:46.482519Z","shell.execute_reply.started":"2024-04-08T01:50:46.47387Z","shell.execute_reply":"2024-04-08T01:50:46.48152Z"},"trusted":true,"id":"VBRiAbXDaomO","executionInfo":{"status":"ok","timestamp":1712553184411,"user_tz":-420,"elapsed":5,"user":{"displayName":"","userId":""}},"outputId":"297bc900-2a2b-4151-bd9c-1bdfd98682e9","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n"]}]},{"cell_type":"code","source":["class FlickrDataset(Dataset):\n","    def __init__(self, data_transform):\n","        self.dataset = pd.read_csv(\"/kaggle/input/flickr8k/captions.txt\")\n","        self.transform = data_transform\n","        self.vocab = {\"<START>\", \"<END>\", \"<PAD>\"}\n","        self.max_line = 0\n","        self.itos = dict()\n","        self.stoi = dict()\n","        self.__load_vocab()\n","\n","    def __len__(self):\n","        return len(self.dataset)\n","\n","    def __load_vocab(self):\n","        for i in range(len(self.dataset)):\n","            line = self.dataset.iloc[i]['caption']\n","            self.max_line = max(self.max_line, len(line.split()))\n","            for word in line.split():\n","                self.vocab.add(word.lower())\n","        self.vocab = list(self.vocab)\n","        self.max_line += 2\n","        for i in range(len(self.vocab)):\n","            self.itos[i] = self.vocab[i]\n","            self.stoi[self.vocab[i]] = i\n","\n","    def __getitem__(self, index):\n","        img_path = self.dataset.iloc[index]['image']\n","        img = Image.open(\"/kaggle/input/flickr8k/Images/\"+img_path)\n","        img = self.transform(img)\n","        caption = self.dataset.iloc[index]['caption']\n","        caption = caption.split()\n","        caption = [self.vocab.index(word.lower()) for word in caption]\n","        caption = [self.vocab.index(\"<START>\")] + \\\n","            caption + [self.vocab.index(\"<END>\")]\n","        if (len(caption) < self.max_line):\n","            caption = caption + \\\n","                [self.vocab.index(\"<PAD>\")] * (self.max_line - len(caption))\n","        caption = torch.tensor(caption)\n","        return img, caption"],"metadata":{"execution":{"iopub.status.busy":"2024-04-08T01:49:19.492927Z","iopub.execute_input":"2024-04-08T01:49:19.493258Z","iopub.status.idle":"2024-04-08T01:49:19.510128Z","shell.execute_reply.started":"2024-04-08T01:49:19.493234Z","shell.execute_reply":"2024-04-08T01:49:19.509219Z"},"trusted":true,"id":"XjRwGhSeaomP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset = Flickr(transform)\n","dataloader = DataLoader(dataset , batch_size = 32 , shuffle = True)\n","# Hyperparameters\n","embed_size = 400\n","hidden_size = 512\n","vocab_size = len(dataset.vocab)\n","num_layers = 2\n","learning_rate = 0.0001\n","num_epochs = 2"],"metadata":{"execution":{"iopub.status.busy":"2024-04-08T01:49:19.511262Z","iopub.execute_input":"2024-04-08T01:49:19.511619Z","iopub.status.idle":"2024-04-08T01:49:21.319182Z","shell.execute_reply.started":"2024-04-08T01:49:19.511587Z","shell.execute_reply":"2024-04-08T01:49:21.318166Z"},"trusted":true,"id":"7INJ-4YoaomP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def show_image(inp, title=None):\n","    \"\"\"Imshow for Tensor.\"\"\"\n","    inp = inp.numpy().transpose((1, 2, 0))\n","    plt.imshow(inp)\n","    if title is not None:\n","        plt.title(title)\n","    plt.pause(0.001)  # pause a bit so that plots are update"],"metadata":{"execution":{"iopub.status.busy":"2024-04-08T01:49:21.32182Z","iopub.execute_input":"2024-04-08T01:49:21.322687Z","iopub.status.idle":"2024-04-08T01:49:21.32923Z","shell.execute_reply.started":"2024-04-08T01:49:21.322649Z","shell.execute_reply":"2024-04-08T01:49:21.32829Z"},"trusted":true,"id":"dWNxWBMwaomQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class EncoderCNN(nn.Module):\n","    def __init__(self):\n","        super(EncoderCNN, self).__init__()\n","        resnet = resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n","        for param in resnet.parameters():\n","            param.requires_grad_(False)\n","\n","        modules = list(resnet.children())[:-2]\n","        self.resnet = nn.Sequential(*modules)\n","\n","\n","    def forward(self, images):\n","        features = self.resnet(images)                                    #(batch_size,2048,7,7)\n","        features = features.permute(0, 2, 3, 1)                           #(batch_size,7,7,2048)\n","        features = features.view(features.size(0), -1, features.size(-1)) #(batch_size,49,2048)\n","        return features\n","\n","#Bahdanau Attention\n","class Attention(nn.Module):\n","    def __init__(self, encoder_dim,decoder_dim,attention_dim):\n","        super(Attention, self).__init__()\n","\n","        self.attention_dim = attention_dim\n","\n","        self.W = nn.Linear(decoder_dim,attention_dim)\n","        self.U = nn.Linear(encoder_dim,attention_dim)\n","\n","        self.A = nn.Linear(attention_dim,1)\n","\n","\n","\n","\n","    def forward(self, features, hidden_state):\n","        u_hs = self.U(features)     #(batch_size,num_layers,attention_dim)\n","        w_ah = self.W(hidden_state) #(batch_size,attention_dim)\n","\n","        combined_states = torch.tanh(u_hs + w_ah.unsqueeze(1)) #(batch_size,num_layers,attemtion_dim)\n","\n","        attention_scores = self.A(combined_states)         #(batch_size,num_layers,1)\n","        attention_scores = attention_scores.squeeze(2)     #(batch_size,num_layers)\n","\n","\n","        alpha = F.softmax(attention_scores,dim=1)          #(batch_size,num_layers)\n","\n","        attention_weights = features * alpha.unsqueeze(2)  #(batch_size,num_layers,features_dim)\n","        attention_weights = attention_weights.sum(dim=1)   #(batch_size,num_layers)\n","\n","        return alpha,attention_weights\n","#Attention Decoder\n","class DecoderRNN(nn.Module):\n","    def __init__(self,embed_size, vocab_size, attention_dim,encoder_dim,decoder_dim,drop_prob=0.3):\n","        super().__init__()\n","\n","        #save the model param\n","        self.vocab_size = vocab_size\n","        self.attention_dim = attention_dim\n","        self.decoder_dim = decoder_dim\n","\n","        self.embedding = nn.Embedding(vocab_size,embed_size)\n","        self.attention = Attention(encoder_dim,decoder_dim,attention_dim)\n","\n","\n","        self.init_h = nn.Linear(encoder_dim, decoder_dim)\n","        self.init_c = nn.Linear(encoder_dim, decoder_dim)\n","        self.lstm_cell = nn.LSTMCell(embed_size+encoder_dim,decoder_dim,bias=True)\n","        self.f_beta = nn.Linear(decoder_dim, encoder_dim)\n","\n","\n","        self.fcn = nn.Linear(decoder_dim,vocab_size)\n","        self.drop = nn.Dropout(drop_prob)\n","\n","\n","\n","    def forward(self, features, captions):\n","\n","        #vectorize the caption\n","        embeds = self.embedding(captions)\n","\n","        # Initialize LSTM state\n","        h, c = self.init_hidden_state(features)  # (batch_size, decoder_dim)\n","\n","        #get the seq length to iterate\n","        seq_length = len(captions[0])-1 #Exclude the last one\n","        batch_size = captions.size(0)\n","        num_features = features.size(1)\n","\n","        preds = torch.zeros(batch_size, seq_length, self.vocab_size).to(device)\n","        alphas = torch.zeros(batch_size, seq_length,num_features).to(device)\n","\n","        for s in range(seq_length):\n","            alpha,context = self.attention(features, h)\n","            lstm_input = torch.cat((embeds[:, s], context), dim=1)\n","            h, c = self.lstm_cell(lstm_input, (h, c))\n","\n","            output = self.fcn(self.drop(h))\n","\n","            preds[:,s] = output\n","            alphas[:,s] = alpha\n","\n","\n","        return preds, alphas\n","\n","    def generate_caption(self,features,max_len=20,vocab=None):\n","        # Inference part\n","        # Given the image features generate the captions\n","\n","        batch_size = features.size(0)\n","        h, c = self.init_hidden_state(features)  # (batch_size, decoder_dim)\n","\n","        alphas = []\n","\n","        #starting input\n","        word = torch.tensor(vocab.stoi['<START>']).view(1,-1).to(device)\n","        embeds = self.embedding(word)\n","\n","\n","        captions = []\n","\n","        for i in range(max_len):\n","            alpha, context = self.attention(features, h)\n","\n","            #store the apla score\n","            alphas.append(alpha.cpu().detach().numpy())\n","            lstm_input = torch.cat((embeds[:, 0], context), dim=1)\n","            h, c = self.lstm_cell(lstm_input, (h, c))\n","            output = self.fcn(self.drop(h))\n","            output = output.view(batch_size,-1)\n","\n","\n","            #select the word with most val\n","            predicted_word_idx = output.argmax(dim=1)\n","\n","            #save the generated word\n","            captions.append(predicted_word_idx.item())\n","\n","            #end if <EOS detected>\n","            if vocab.itos[predicted_word_idx.item()] == \"<END>\":\n","                break\n","\n","            #send generated word as the next caption\n","            embeds = self.embedding(predicted_word_idx.unsqueeze(0))\n","\n","        #covert the vocab idx to words and return sentence\n","        return [vocab.itos[idx] for idx in captions],alphas\n","\n","\n","    def init_hidden_state(self, encoder_out):\n","        mean_encoder_out = encoder_out.mean(dim=1)\n","        h = self.init_h(mean_encoder_out)  # (batch_size, decoder_dim)\n","        c = self.init_c(mean_encoder_out)\n","        return h, c"],"metadata":{"execution":{"iopub.status.busy":"2024-04-08T01:51:13.457613Z","iopub.execute_input":"2024-04-08T01:51:13.458029Z","iopub.status.idle":"2024-04-08T01:51:13.484634Z","shell.execute_reply.started":"2024-04-08T01:51:13.457986Z","shell.execute_reply":"2024-04-08T01:51:13.483529Z"},"trusted":true,"id":"aoY9FHBXaomQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class EncoderDecoder(nn.Module):\n","    def __init__(self,embed_size, vocab_size, attention_dim,encoder_dim,decoder_dim,drop_prob=0.3):\n","        super().__init__()\n","        self.encoder = EncoderCNN()\n","        self.decoder = DecoderRNN(\n","            embed_size=embed_size,\n","            vocab_size = len(dataset.vocab),\n","            attention_dim=attention_dim,\n","            encoder_dim=encoder_dim,\n","            decoder_dim=decoder_dim\n","        )\n","\n","    def forward(self, images, captions):\n","        features = self.encoder(images)\n","        outputs = self.decoder(features, captions)\n","        return outputs"],"metadata":{"execution":{"iopub.status.busy":"2024-04-08T01:49:21.360153Z","iopub.execute_input":"2024-04-08T01:49:21.360551Z","iopub.status.idle":"2024-04-08T01:49:21.373854Z","shell.execute_reply.started":"2024-04-08T01:49:21.360515Z","shell.execute_reply":"2024-04-08T01:49:21.373039Z"},"trusted":true,"id":"EGIO_bx0aomQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Hyperparams\n","embed_size=300\n","vocab_size = len(dataset.vocab)\n","attention_dim=256\n","encoder_dim=2048\n","decoder_dim=512\n","learning_rate = 3e-4\n","#init model\n","model = EncoderDecoder(\n","    embed_size=300,\n","    vocab_size = len(dataset.vocab),\n","    attention_dim=256,\n","    encoder_dim=2048,\n","    decoder_dim=512\n",").to(device)\n","\n","criterion = nn.CrossEntropyLoss(ignore_index=dataset.stoi[\"<PAD>\"])\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","num_epochs = 20\n","print_every = 100\n","model.train()\n","for epoch in range(1,num_epochs+1):\n","    for idx, (image, captions) in enumerate(iter(dataloader)):\n","        image,captions = image.to(device),captions.to(device)\n","\n","        # Zero the gradients.\n","        optimizer.zero_grad()\n","\n","        # Feed forward\n","        outputs,attentions = model(image, captions)\n","\n","        # Calculate the batch loss.\n","        targets = captions[:,1:]\n","        loss = criterion(outputs.view(-1, vocab_size), targets.reshape(-1))\n","\n","        # Backward pass.\n","        loss.backward()\n","\n","        # Update the parameters in the optimizer.\n","        optimizer.step()\n","\n","        if (idx+1)%print_every == 0:\n","            print(\"Epoch: {} loss: {:.5f}\".format(epoch,loss.item()))\n","\n","\n","            #generate the caption\n","            model.eval()\n","            with torch.no_grad():\n","                dataiter = iter(dataloader)\n","                img,_ = next(dataiter)\n","                features = model.encoder(img[0:1].to(device))\n","                caps, _ = model.decoder.generate_caption(features,vocab=dataset)\n","                caption = ' '.join(caps)\n","                show_image(img[0].cpu(),title=caption)\n","\n","            model.train()"],"metadata":{"execution":{"iopub.status.busy":"2024-04-08T01:52:22.845246Z","iopub.execute_input":"2024-04-08T01:52:22.84601Z","iopub.status.idle":"2024-04-08T01:52:44.503242Z","shell.execute_reply.started":"2024-04-08T01:52:22.845981Z","shell.execute_reply":"2024-04-08T01:52:44.501567Z"},"trusted":true,"id":"dbc6NYPRaomR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"EHFn0KTTaomR"},"execution_count":null,"outputs":[]}]}